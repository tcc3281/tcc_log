# ðŸ“š LM Studio AI Backend Documentation

## ðŸŽ¯ Tá»•ng quan

File `lm_studio.py` lÃ  trung tÃ¢m xá»­ lÃ½ AI cá»§a á»©ng dá»¥ng, Ä‘Æ°á»£c thiáº¿t káº¿ Ä‘á»ƒ quáº£n lÃ½ táº¥t cáº£ cÃ¡c tÆ°Æ¡ng tÃ¡c vá»›i AI models thÃ´ng qua LM Studio vÃ  OpenAI API. File nÃ y Ä‘Ã£ Ä‘Æ°á»£c refactor hoÃ n toÃ n Ä‘á»ƒ sá»­ dá»¥ng **streaming responses** cho táº¥t cáº£ cÃ¡c cuá»™c trÃ² chuyá»‡n vÃ  tÃ­ch há»£p **intelligent SQL processing** vá»›i auto-execution capabilities.

## ðŸ—ï¸ Kiáº¿n trÃºc tá»•ng thá»ƒ

```mermaid
graph TB
    subgraph "User Interface Layer"
        A["User Chat Input"]
        B["Analysis Request"]
        C["Writing Improvement"]
        D["Prompt Generation"]
    end
    
    subgraph "Main Chat Orchestrator"
        E["chat_with_ai() - CENTRAL HUB"]
        E --> F["Agent Mode?"]
        E --> G["Database Context?"]
        E --> H["Streaming Mode?"]
    end
    
    subgraph "AI Processing Modes"
        F -->|Yes| I["LangChain Agent Mode"]
        F -->|No| J["Direct LLM Mode"]
        
        I --> K["Agent with Tools"]
        I --> L["SQL Auto-execution"]
        
        J --> M["Enhanced System Prompt"]
        J --> N["Schema Injection"]
    end
    
    subgraph "Core Streaming Engine"
        O["query_lm_studio_stream()"]
        O --> P["Direct OpenAI API"]
        P --> Q["Real-time Token Streaming"]
        Q --> R["Performance Stats"]
    end
    
    subgraph "LM Studio Integration"
        S["LM Studio Local Server"]
        S --> T["Model Management"]
        S --> U["Health Monitoring"]
        S --> V["Auto Model Selection"]
    end
    
    subgraph "SQL Intelligence System"
        W["SQL Post-processing"]
        W --> X["Pattern Recognition"]
        X --> Y["Query Extraction"]
        Y --> Z["Auto Execution"]
        Z --> AA["Result Formatting"]
    end
    
    A --> E
    B --> E
    C --> E
    D --> E
    
    K --> O
    M --> O
    O --> S
    
    L --> W
    
    style E fill:#1C3C3C,color:#ffffff
    style S fill:#ff6b35
    style O fill:#005571
    style W fill:#316192
```

### ðŸ”„ Request Processing Flow

```mermaid
sequenceDiagram
    participant User
    participant ChatAI as "chat_with_ai()"
    participant Agent as "LangChain Agent"
    participant LM as "LM Studio"
    participant SQL as "SQL Processor"
    participant DB as "Database"
    
    User->>ChatAI: "Send Message"
    ChatAI->>ChatAI: "Detect Context (DB/General)"
    
    alt Agent Mode
        ChatAI->>Agent: "Initialize with Tools"
        Agent->>LM: "Stream with Tool Access"
        LM-->>Agent: "Response with SQL"
        Agent-->>ChatAI: "Complete Response"
        ChatAI->>SQL: "Post-process SQL"
        SQL->>DB: "Execute Queries"
        DB-->>SQL: "Results"
        SQL-->>User: "Enhanced Response"
    else Direct Mode
        ChatAI->>ChatAI: "Enhance System Prompt"
        ChatAI->>LM: "Direct Streaming Call"
        LM-->>ChatAI: "Token Stream"
        ChatAI-->>User: "Real-time Response"
    end
    
    Note over ChatAI,User: "Streaming throughout entire process"
```

## ðŸ“Š Data Models & Architecture

### ðŸ—ï¸ Core Data Models

```mermaid
classDiagram
    class AIMessage {
        +str role
        +str content
        +validate_role()
    }
    
    class AIRequest {
        +List[AIMessage] messages
        +Optional[str] model
        +Optional[float] temperature
        +Optional[int] max_tokens
        +Optional[List[Dict]] tools
        +validate_messages()
        +set_defaults()
    }
    
    class AIResponse {
        +str content
        +str model
        +Optional[Dict] usage
        +Optional[float] tokens_per_second
        +Optional[float] time_to_first_token
        +Optional[List[Dict]] tool_calls
        +calculate_performance()
    }
    
    class ParsedAIResponse {
        +Optional[str] think
        +str answer
        +str raw_content
        +separate_thinking()
    }
    
    AIRequest --> "1..*" AIMessage
    AIResponse --> ParsedAIResponse : "parse_ai_response()"
    
    note for AIMessage "Roles: system, user, assistant"
    note for ParsedAIResponse "Separates <think> sections from final answer"
```

### ðŸ”§ Core Management Functions

```mermaid
graph TD
    subgraph "Instance Management"
        A["get_chatopen_ai_instance()"]
        A --> B["Parameters Changed?"]
        B -->|Yes| C["Create New Instance"]
        B -->|No| D["Return Cached Instance"]
        C --> E["ChatOpenAI Instance"]
        D --> E
    end
    
    subgraph "Model Management"
        F["get_available_models()"]
        F --> G["Cache Check"]
        G --> H["Cache Valid?"]
        H -->|Yes| I["Return Cached Models"]
        H -->|No| J["Query LM Studio API"]
        J --> K["Update Cache"]
        K --> L["Return Model List"]
    end
    
    subgraph "Smart Model Selection"
        M["validate_and_get_model()"]
        M --> N["Model Valid?"]
        N -->|No| O["Get Available Models"]
        O --> P["Filter Non-Embedding"]
        P --> Q["Select Best Model"]
        N -->|Yes| R["Return Model"]
        Q --> R
    end
    
    style A fill:#1C3C3C,color:#ffffff
    style F fill:#ff6b35
    style M fill:#005571
```

### ðŸŒŠ Streaming Architecture Deep Dive

```mermaid
graph TB
    subgraph "Streaming Entry Point"
        A["query_lm_studio_stream()"]
        A --> B["Validate Model"]
        A --> C["Setup Parameters"]
        A --> D["Initialize OpenAI Client"]
    end
    
    subgraph "Streaming Process"
        D --> E["Create Stream Request"]
        E --> F["Process Chunks Async"]
        F --> G["Chunk Type?"]
        
        G -->|Content| H["Extract Content"]
        G -->|Stats| I["Calculate Performance"]
        G -->|Error| J["Handle Error"]
        
        H --> K["Yield Content Token"]
        I --> L["Yield Stats JSON"]
        J --> M["Yield Error Message"]
    end
    
    subgraph "Performance Tracking"
        N["Start Time"]
        O["End Time"]
        P["Content Length"]
        Q["Calculate Tokens/Second"]
        
        N --> Q
        O --> Q
        P --> Q
        Q --> L
    end
    
    K --> R["Client Receives Real-time"]
    L --> S["Client Updates Stats"]
    M --> T["Client Shows Error"]
    
    style A fill:#1C3C3C,color:#ffffff
    style K fill:#005571
    style L fill:#ff6b35
```

## ðŸ¤– Central Chat Orchestrator - chat_with_ai()

### ðŸŽ¯ Main Decision Flow

```mermaid
graph TD
    A["chat_with_ai() - ENTRY POINT"] --> B["Database Context Detection"]
    
    B -->|"DB Keywords Found"| C["Load SQL Schema"]
    B -->|"No DB Keywords"| D["Standard Processing"]
    
    C --> E["Inject Schema into Prompt"]
    D --> F["Use Base System Prompt"]
    E --> F
    
    F --> G["use_agent = True?"]
    
    G -->|Yes| H["LangChain Agent Mode"]
    G -->|No| I["Direct LLM Mode"]
    
    subgraph "Agent Mode Processing"
        H --> J["Initialize LangChain Agent"]
        J --> K["Add SQL Tools"]
        K --> L["Streaming?"]
        L -->|Yes| M["Agent Streaming"]
        L -->|No| N["Agent Non-streaming"]
        
        M --> O["Post-process SQL"]
        N --> O
    end
    
    subgraph "Direct Mode Processing"
        I --> P["Enhanced System Prompt"]
        P --> Q["Streaming?"]
        Q -->|Yes| R["query_lm_studio_stream()"]
        Q -->|No| S["query_lm_studio()"]
    end
    
    O --> T["SQL Auto-execution"]
    R --> U["Real-time Response"]
    S --> V["Complete Response"]
    T --> W["Enhanced Results"]
    
    style A fill:#1C3C3C,color:#ffffff
    style H fill:#ff6b35
    style I fill:#005571
    style T fill:#316192
```

### ðŸ” Database Context Detection

```mermaid
graph LR
    A["User Message"] --> B["Keyword Detection"]
    
    B --> C["Contains DB Keywords?"]
    
    subgraph "DB Keywords"
        D["database"]
        E["sql"] 
        F["query"]
        G["table"]
        H["schema"]
        I["select/insert/update/delete"]
        J["join/where"]
        K["postgres/postgresql"]
    end
    
    C -->|Yes| L["Load PostgreSQL Tool"]
    C -->|No| M["Skip DB Integration"]
    
    L --> N["Get Database Schema"]
    N --> O["Format Schema for AI"]
    O --> P["Inject into System Prompt"]
    
    M --> Q["Standard Chat Processing"]
    P --> R["Enhanced DB-aware Chat"]
    
    style L fill:#316192
    style R fill:#1C3C3C,color:#ffffff
```

## ðŸ› ï¸ Specialized AI Functions

### ðŸ“Š Journal Analysis System

```mermaid
graph TD
    A["analyze_journal_entry()"] --> B["Select Analysis Type"]
    
    B --> C["Analysis Type"]
    
    C -->|general| D["General Analysis - 800 tokens"]
    C -->|mood| E["Mood Analysis - 800 tokens"]
    C -->|summary| F["Content Summary - 600 tokens"]
    C -->|insights| G["Learning Insights - 700 tokens"]
    
    D --> H["Get Analysis Prompt"]
    E --> H
    F --> H
    G --> H
    
    H --> I["Format Entry Content"]
    I --> J["Create AI Request"]
    J --> K["process_ai_request()"]
    K --> L["Parse Response"]
    L --> M["Return Analysis Result"]
    
    subgraph "Analysis Prompts"
        N["Prompt Manager"]
        N --> O["general_analysis.prompt"]
        N --> P["mood_analysis.prompt"]
        N --> Q["summary_analysis.prompt"]
        N --> R["insights_analysis.prompt"]
    end
    
    H --> N
    
    style K fill:#1C3C3C,color:#ffffff
    style N fill:#ff6b35
```

### âœï¸ Writing Enhancement System

```mermaid
graph TD
    A["improve_writing()"] --> B["Select Improvement Type"]
    
    B --> C["Improvement Type"]
    
    C -->|grammar| D["Grammar Correction"]
    C -->|style| E["Style Improvement"]
    C -->|clarity| F["Clarity Enhancement"]
    C -->|tone| G["Tone Adjustment"]
    
    D --> H["Get Writing Prompt"]
    E --> H
    F --> H
    G --> H
    
    H --> I["Low Temperature 0.3"]
    I --> J["Max 1500 tokens"]
    J --> K["Create AI Request"]
    K --> L["process_ai_request()"]
    L --> M["Return Improved Text"]
    
    subgraph "Writing Prompts"
        N["Prompt Manager"]
        N --> O["grammar_improvement.prompt"]
        N --> P["style_improvement.prompt"]
        N --> Q["clarity_improvement.prompt"]
        N --> R["tone_adjustment.prompt"]
    end
    
    H --> N
    
    style L fill:#1C3C3C,color:#ffffff
    style I fill:#005571
```
## ðŸ” SQL Intelligence System - Auto-Execution Engine

### ðŸŽ¯ SQL Post-Processing Architecture

```mermaid
graph TB
    subgraph "SQL Detection & Extraction"
        A["AI Response Content"] --> B["SQL Pattern Detection"]
        B --> C["Contains SQL?"]
        
        C -->|Yes| D["Multiple Pattern Matching"]
        C -->|No| E["Return Original Response"]
        
        subgraph "SQL Patterns"
            F["Code Block with SQL"]
            G["SQL Code Block"]
            H["Inline SQL Block"]
        end
        
        D --> F
        D --> G
        D --> H
    end
    
    subgraph "Query Validation & Cleaning"
        I["Extract SQL Query"] --> J["Clean Query"]
        J --> K["Remove Comments"]
        K --> L["Validate SQL Keywords"]
        L --> M["Valid SQL?"]
        
        M -->|Yes| N["Check for Duplicates"]
        M -->|No| O["Skip Query"]
        
        N --> P["Already Executed?"]
        P -->|Yes| Q["Skip Duplicate"]
        P -->|No| R["Proceed to Execution"]
    end
    
    subgraph "Execution & Formatting"
        R --> S["Execute SQL Query"]
        S --> T["Execution Success?"]
        
        T -->|Yes| U["Format Results"]
        T -->|No| V["Log Error"]
        
        U --> W["Result Type?"]
        W -->|"Single Value"| X["Format as Value"]
        W -->|"Multiple Rows"| Y["Format as Table"]
        
        X --> Z["Return Enhanced Response"]
        Y --> Z
    end
    
    F --> I
    G --> I
    H --> I
    
    style R fill:#316192
    style S fill:#1C3C3C,color:#ffffff
    style Z fill:#005571
```

### ðŸ”„ SQL Execution Flow

```mermaid
sequenceDiagram
    participant AI as "AI Response"
    participant Processor as "SQL Processor"
    participant Validator as "Query Validator"
    participant DB as "Database"
    participant Formatter as "Result Formatter"
    
    AI->>Processor: "Response with SQL code blocks"
    Processor->>Processor: "Extract SQL patterns"
    
    loop "For each SQL query found"
        Processor->>Validator: "Clean and validate query"
        Validator->>Validator: "Check syntax and keywords"
        
        alt "Query is valid"
            Validator->>DB: "Execute SQL query"
            DB-->>Validator: "Query results"
            Validator->>Formatter: "Format results"
            
            alt "Single value result"
                Formatter-->>Processor: "âœ… REAL Result: value"
            else "Multiple rows"
                Formatter-->>Processor: "âœ… REAL Results: table"
            end
        else "Query invalid"
            Validator-->>Processor: "Skip invalid query"
        end
    end
    
    Processor-->>AI: "Enhanced response with real results"
```

### ðŸ§¹ SQL Query Cleaning Process

```mermaid
graph TD
    A[Raw SQL from AI] --> B[Remove Leading/Trailing Whitespace]
    B --> C[Split into Lines]
    C --> D[Filter Comments & Empty Lines]
    
    D --> E["Line Processing"]
    E -->|Comment Line| F["Skip Line"]
    E -->|Empty Line| F
    E -->|Valid Line| G[Keep Line]
    
    F --> H[Continue Next Line]
    G --> H
    H --> I["More Lines?"]
    
    I -->|Yes| E
    I -->|No| J[Join Valid Lines]
    
    J --> K[Remove Extra Whitespace]
    K --> L[Add Semicolon if Missing]
    L --> M[Final Cleaned Query]
    
    style M fill:#1C3C3C,color:#ffffff
```

### ðŸ“Š Result Formatting System

```mermaid
graph TD
    A[Query Results] --> B["Result Type Check"]
    
    B -->|Empty Results| C["Return No data found"]
    B -->|Single Value| D[Format Single Value]
    B -->|Multiple Rows| E[Format Table]
    
    subgraph "Single Value Formatting"
        D --> F[Extract Value]
        F --> G["Return REAL Result with value"]
    end
    
    subgraph "Table Formatting"
        E --> H[Get Column Names]
        H --> I[Filter Sensitive Columns]
        I --> J["Limit Columns (max 6)"]
        J --> K[Create Markdown Table]
        
        K --> L[Add Table Headers]
        L --> M[Add Separator Row]
        M --> N["Add Data Rows (max 10)"]
        N --> O[Format Cell Values]
        
        subgraph "Cell Formatting"
            O --> P["Value Type?"]
            P -->|null| Q["null value"]
            P -->|datetime| R["YYYY-MM-DD HH:MM"]
            P -->|long string| S["value truncated"]
            P -->|boolean| T["âœ“/âœ—"]
            P -->|other| U["str(value)"]
        end
        
        O --> V[Escape Markdown Characters]
        V --> W[Return Formatted Table]
    end
    
    style G fill:#005571
    style W fill:#316192
```

### ðŸ›¡ï¸ Security & Validation

```mermaid
graph LR
    A[SQL Query] --> B[Keyword Validation]
    
    B --> C["Starts with Valid Keyword?"]
    
    subgraph "Valid Keywords"
        D[SELECT]
        E[INSERT]
        F[UPDATE]
        G[DELETE]
        H[CREATE]
        I[DROP]
        J[ALTER]
        K[SHOW/DESCRIBE]
    end
    
    C -->|Yes| L[Check Forbidden Content]
    C -->|No| M[Reject Query]
    
    L --> N["Contains Forbidden?"]
    
    subgraph "Forbidden Content"
        O[example/placeholder]
        P[your_table/your_column]
        Q[sample_data]
        R[explanatory text mixed]
    end
    
    N -->|Yes| S[Reject Query]
    N -->|No| T[Length Validation]
    
    T --> U["Query Length > 8?"]
    U -->|Yes| V[Accept Query]
    U -->|No| W[Reject Query]
    
    style V fill:#1C3C3C,color:#ffffff
    style M fill:#ff4444
    style S fill:#ff4444
    style W fill:#ff4444
```
```python
async def validate_and_get_model(model: Optional[str] = None) -> str:
    """Validate and get the best available model"""
    target_model = model or AI_MODEL
    
    # If model is placeholder or invalid, get the first available model
    if target_model == "your-model-identifier" or not target_model or target_model == DEFAULT_AI_MODEL:
        try:
            available_models = await get_available_models()
            if available_models:
                # Use the first available model that's not an embedding model
                for available_model in available_models:
                    if "embedding" not in available_model.lower():
                        target_model = available_model
                        break
                if not target_model or target_model == "your-model-identifier":
                    target_model = available_models[0]  # Fallback to first model
                logger.info(f"Auto-selected model: {target_model}")
            else:
                logger.warning("No models available in LM Studio")
        except Exception as e:
            logger.warning(f"Could not get available models: {e}, using configured model: {target_model}")
    
    return target_model
```

**TÃ­nh nÄƒng chÃ­nh**:
- **Auto-selection**: Tá»± Ä‘á»™ng chá»n model phÃ¹ há»£p náº¿u khÃ´ng chá»‰ Ä‘á»‹nh
- **Embedding Filter**: Loáº¡i bá» embedding models khá»i lá»±a chá»n
- **Fallback Strategy**: CÃ³ backup plan náº¿u model khÃ´ng available

### 5. **Core Query Functions**

#### `query_lm_studio_internal()` - LangChain Integration
```python
async def query_lm_studio_internal(request: AIRequest, timeout: float = None) -> AIResponse:
    """Internal function to send a request to LM Studio API using LangChain"""
    model = await validate_and_get_model(request.model)
    temperature = request.temperature or DEFAULT_TEMPERATURE
    max_tokens = request.max_tokens or DEFAULT_MAX_TOKENS
    
    # Get LangChain model instance
    llm = get_chatopen_ai_instance(model, temperature, max_tokens)
    
    # Convert our AIMessage objects to LangChain message objects
    langchain_messages = []
    for msg in request.messages:
        if msg.role == "system":
            langchain_messages.append(SystemMessage(content=msg.content))
        elif msg.role == "user":
            langchain_messages.append(HumanMessage(content=msg.content))
        elif msg.role == "assistant":
            langchain_messages.append(LCMessage(content=msg.content))
    
    try:
        import time
        start_time = time.time()
        
        # Invoke the model
        response = await llm.ainvoke(langchain_messages)
        
        end_time = time.time()
        total_time = end_time - start_time
        
        # Extract content from LangChain response
        full_content = response.content
        
        # Create response object with collected data
        ai_response = AIResponse(
            content=full_content,
            model=model,
            usage=None,  # Not directly available from LangChain
            tokens_per_second=None,
            time_to_first_token=None,
            tool_calls=None
        )
        return ai_response
                
    except Exception as e:
        logger.error(f"Error querying LM Studio API: {e}")
        raise
```

**TÃ­nh nÄƒng chÃ­nh**:
- **LangChain Integration**: Sá»­ dá»¥ng LangChain Ä‘á»ƒ communicate vá»›i LM Studio
- **Message Conversion**: Convert between custom format vÃ  LangChain format
- **Performance Tracking**: Äo thá»i gian inference
- **Error Handling**: Comprehensive error management

#### `query_lm_studio()` - Retry Logic
```python
async def query_lm_studio(request: AIRequest, max_retries: int = 3) -> AIResponse:
    """Query LM Studio with retry logic using LangChain"""
    retry_count = 0
    last_error = None
    
    # Set timeout based on environment configuration - convert from milliseconds to seconds
    timeout = MAX_INFERENCE_TIME / 1000
    
    while retry_count < max_retries:
        try:
            response = await query_lm_studio_internal(request, timeout=timeout)
            return response
        except Exception as e:
            last_error = e
            if "Client disconnected" in str(e):
                logger.warning(f"LM Studio disconnected (attempt {retry_count + 1}/{max_retries}), retrying...")
                retry_count += 1
                await asyncio.sleep(1)  # Wait 1 second before retry
            else:
                raise e
    
    raise last_error
```

**TÃ­nh nÄƒng chÃ­nh**:
- **Retry Mechanism**: Tá»‘i Ä‘a 3 láº§n retry cho connection failures
- **Smart Error Detection**: PhÃ¢n biá»‡t loáº¡i lá»—i Ä‘á»ƒ quyáº¿t Ä‘á»‹nh retry
- **Exponential Backoff**: Wait time giá»¯a cÃ¡c retry attempts

### 6. **ðŸŒŠ Streaming Implementation - CORE FEATURE**

#### `query_lm_studio_stream()` - Main Streaming Function
```python
async def query_lm_studio_stream(request: AIRequest):
    """Query LM Studio with streaming response using direct OpenAI client"""
    try:
        import time
        import json
        start_time = time.time()
        
        # Set up parameters
        model = await validate_and_get_model(request.model)
        temperature = request.temperature if request.temperature is not None else DEFAULT_TEMPERATURE
        max_tokens = request.max_tokens if request.max_tokens is not None else DEFAULT_MAX_TOKENS
        
        # Set timeout from configuration (convert from ms to seconds)
        timeout = MAX_INFERENCE_TIME / 1000
        
        # Use direct OpenAI API streaming since LangChain streaming has issues
        from openai import AsyncOpenAI
        
        # Initialize OpenAI client for LM Studio
        openai_client = AsyncOpenAI(
            base_url=LM_STUDIO_BASE_URL,
            api_key="not-needed"  # LM Studio doesn't require an API key
        )
        
        # Convert our AIMessage objects to OpenAI format
        openai_messages = []
        for msg in request.messages:
            openai_messages.append({"role": msg.role, "content": msg.content})
        
        # Stream response directly using OpenAI client
        stream = await openai_client.chat.completions.create(
            model=model,
            messages=openai_messages,
            temperature=temperature,
            max_tokens=max_tokens,
            stream=True,
            timeout=timeout
        )
        
        collected_content = ""
        async for chunk in stream:
            if hasattr(chunk, 'choices') and chunk.choices:
                choice = chunk.choices[0]
                if hasattr(choice, 'delta') and hasattr(choice.delta, 'content') and choice.delta.content is not None:
                    content = choice.delta.content
                    collected_content += content
                    yield content
        
        # Calculate final stats
        end_time = time.time()
        total_time = end_time - start_time
        
        # Estimate tokens based on content length (approximate)
        total_tokens = len(collected_content) / 4
        tokens_per_second = total_tokens / total_time if total_time > 0 else 0
        
        # Send stats as a separate message
        yield json.dumps({
            "type": "stats",
            "inference_time": int(total_time * 1000),  # Convert to milliseconds
            "tokens_per_second": tokens_per_second
        })
    
    except Exception as e:
        logger.error(f"Error in streaming query: {str(e)}")
        yield f"Error: {str(e)}"
```

**TÃ­nh nÄƒng chÃ­nh**:
- **True Token-level Streaming**: Yield content ngay khi nháº­n Ä‘Æ°á»£c tá»« model
- **Direct OpenAI API**: Bypass LangChain streaming issues
- **Performance Stats**: Theo dÃµi inference time vÃ  tokens/second
- **Error Recovery**: Graceful error handling vá»›i meaningful messages
- **Memory Efficient**: KhÃ´ng buffer toÃ n bá»™ response

### 7. **Specialized AI Functions**

#### `analyze_journal_entry()` - Journal Analysis
```python
async def analyze_journal_entry(
    entry_title: str, 
    entry_content: str, 
    analysis_type: str = "general", 
    model: Optional[str] = None
) -> Dict[str, Any]:
    """Analyze a journal entry using the AI model."""
    system_prompt = get_prompt_manager().get_analysis_prompt(analysis_type)
    max_tokens = ANALYSIS_MAX_TOKENS.get(analysis_type, 800)
    
    content = f"Journal title: {entry_title}\n\nJournal content:\n{entry_content}"
    
    result = await process_ai_request(
        content=content,
        system_prompt=system_prompt,
        model=model,
        temperature=0.7,
        max_tokens=max_tokens,
        task_name="journal analysis"
    )
    
    result["analysis_type"] = analysis_type
    return result
```

**Analysis Types**:
- `general`: PhÃ¢n tÃ­ch tá»•ng quÃ¡t (800 tokens)
- `mood`: PhÃ¢n tÃ­ch tÃ¢m tráº¡ng (800 tokens)
- `summary`: TÃ³m táº¯t ná»™i dung (600 tokens)
- `insights`: Insights vÃ  recommendations (700 tokens)

#### `improve_writing()` - Writing Enhancement
```python
async def improve_writing(
    text: str,
    improvement_type: str = "grammar",
    model: Optional[str] = None
) -> Dict[str, Any]:
    """Improve the writing quality of English text."""
    system_prompt = get_prompt_manager().get_writing_improvement_prompt(improvement_type)
    
    content = f"Please improve this text:\n\n{text}"
    
    result = await process_ai_request(
        content=content,
        system_prompt=system_prompt,
        model=model,
        temperature=0.3,  # Lower temperature for more consistent improvements
        max_tokens=1500,
        task_name="writing improvement"
    )
    
    result["improvement_type"] = improvement_type
    result["original_text"] = text
    return result
```

**Improvement Types**:
- `grammar`: Grammar correction
- `style`: Style improvement
- `clarity`: Clarity enhancement
- `tone`: Tone adjustment

#### `generate_journaling_prompts()` - Creative Prompts
```python
async def generate_journaling_prompts(
    topic: str = "",
    theme: str = "",
    count: int = 5,
    model: Optional[str] = None
) -> List[str]:
    """Generate journaling prompts using the AI model."""
    base_content = f"Generate {count} journaling prompts"
    if topic:
        base_content += f" about {topic}"
    if theme:
        base_content += f" with theme {theme}"
    base_content += "."
    
    try:
        result = await process_ai_request(
            content=base_content,
            system_prompt=get_system_prompt("journaling_prompts"),
            model=model,
            temperature=0.8,  # Higher temperature for creativity
            max_tokens=500,
            task_name="journaling prompts"
        )
        
        # Process response - extract bullet points
        content = result["answer"]
        
        # Split by newlines and extract bullet points
        lines = content.split('\n')
        prompts = [line.strip()[2:].strip() if line.strip().startswith('-') else line.strip() 
                   for line in lines if line.strip()]
        
        # Filter out any non-prompt text
        prompts = [p for p in prompts if len(p) >= 10]  # Only keep substantive prompts
        
        # Limit to requested count
        return prompts[:count]
    except Exception as e:
        logger.error(f"Error generating journaling prompts: {e}")
        return [f"Error generating journaling prompts: {str(e)}"]
```

**TÃ­nh nÄƒng chÃ­nh**:
- **Creative Temperature**: Sá»­ dá»¥ng temperature 0.8 cho creativity
- **Flexible Parameters**: Topic vÃ  theme customization
- **Content Processing**: Extract vÃ  clean prompt list
- **Error Resilience**: Fallback vá»›i error message

### 8. **ðŸ¤– Main Chat Function - CENTRAL ORCHESTRATOR**

#### `chat_with_ai()` - Main Chat Interface
```python
async def chat_with_ai(
    message: str,
    history: List[Dict[str, str]] = None,
    model: Optional[str] = None,
    system_prompt: Optional[str] = None,
    streaming: bool = False,
    use_agent: bool = False,
    tools: Optional[List[Tool]] = None
):
    """Process a chat message with AI."""
```

**Logic Flow**:

1. **Database Context Detection**:
```python
# Kiá»ƒm tra message cÃ³ liÃªn quan Ä‘áº¿n database khÃ´ng
is_db_related = any(keyword in message.lower() for keyword in 
                   ["database", "sql", "query", "table", "schema", "select", "insert", 
                   "update", "delete", "join", "where", "postgres", "postgresql"])

db_schema_prompt = ""
# Láº¥y thÃ´ng tin database schema náº¿u cáº§n
if is_db_related:
    try:
        from app.ai.sql_tool import PostgreSQLTool
        database_url = os.getenv("DATABASE_URL")
        if database_url:
            pg_tool = PostgreSQLTool()
            schema_result = pg_tool.get_database_schema()
            
            if schema_result.get("success", False):
                schema_text = schema_result.get("schema", "")
                # Táº¡o enhanced prompt vá»›i database schema
                db_schema_prompt = schema_prompt_prefix + schema_text + schema_prompt_suffix
```

2. **Agent vs Non-Agent Mode**:
```python
if use_agent:
    # Agent Mode vá»›i LangChain Agent
    try:
        from app.ai.agent import LangChainAgent
        agent = LangChainAgent(
            model_name=model or AI_MODEL,
            system_prompt=system_prompt,
            tools=tools
        )
        
        if streaming:
            # Agent streaming vá»›i tool access
            collected_content = ""
            async for chunk in agent.chat_with_agent_streaming(message):
                if chunk:
                    collected_content += chunk
                    yield chunk
            
            # Post-process SQL execution
            real_sql_result = await _post_process_sql_execution(collected_content, streaming=True)
            if real_sql_result and real_sql_result.get("success", False):
                yield f"\n\n{real_sql_result.get('message', '')}"
        else:
            # Non-streaming agent mode
            async for response in agent.chat(message, streaming=False):
                if response:
                    yield response
    except Exception as agent_error:
        # Fallback to non-agent mode
        logger.error(f"Agent error: {agent_error}")
        # Continue to non-agent processing...
```

3. **Non-Agent Mode (Standard Chat)**:
```python
# Non-agent mode vá»›i enhanced system prompt
base_system_prompt = system_prompt or get_system_prompt("default_chat")

# ThÃªm database schema náº¿u cáº§n
if db_schema_prompt and is_db_related:
    enhanced_system_prompt = base_system_prompt + db_schema_prompt
else:
    enhanced_system_prompt = base_system_prompt

request = await create_ai_request(
    content=message,
    system_prompt=enhanced_system_prompt,
    model=model,
    temperature=0.7,
    max_tokens=2000,
    history=history
)

if streaming:
    async for chunk in query_lm_studio_stream(request):
        yield chunk
else:
    response = await query_lm_studio(request)
    yield {
        "content": response.content,
        "model": response.model,
        "usage": response.usage
    }
```

**TÃ­nh nÄƒng chÃ­nh**:
- **Smart Context Injection**: Tá»± Ä‘á»™ng thÃªm database schema cho DB queries
- **Agent Fallback**: Graceful fallback tá»« agent sang non-agent mode
- **Universal Streaming**: Cáº£ agent vÃ  non-agent Ä‘á»u support streaming
- **SQL Post-processing**: Tá»± Ä‘á»™ng execute SQL tá»« agent responses
- **History Management**: Maintain conversation context

### 9. **ðŸ”§ SQL Post-Processing System**

#### `_post_process_sql_execution()` - Auto SQL Execution
```python
async def _post_process_sql_execution(content: str, streaming: bool = False):
    """Post-process agent response to execute SQL code if provided but not executed"""
    try:
        from app.ai.sql_tool import PostgreSQLTool
        import re, os
        
        # Initialize SQL tool
        database_url = os.getenv("DATABASE_URL")
        if not database_url:
            return None
            
        sql_tool = PostgreSQLTool(database_url)
        
        # SQL extraction patterns (priority order)
        sql_patterns = [
            r'```\s*\n([^`]+)\n```',      # ``` SQL QUERY ``` (new format)
            r'```sql\s*\n([^`]+)\n```',   # ```sql SELECT ... ``` (old format)
            r'```([^`]+)```',             # ```SQL QUERY``` (single line)
        ]
        
        # Check if response already contains real results
        has_real_results = any(indicator in content.lower() for indicator in [
            'rows returned', 'query executed successfully', 'actual count', 'real count',
            'query result:', 'execution completed', 'data retrieved'
        ])
        
        if has_real_results:
            return None  # Skip if already has real results
        
        # Extract and execute SQL queries
        sql_executed = False
        executed_queries = set()  # Avoid duplicates
        
        for pattern in sql_patterns:
            matches = re.findall(pattern, content, re.IGNORECASE | re.MULTILINE | re.DOTALL)
            
            for match in matches:
                sql_query = _clean_sql_query(match)
                if not sql_query:
                    continue
                
                # Skip duplicates
                query_key = sql_query.strip().lower().replace(' ', '').replace('\n', '')
                if query_key in executed_queries:
                    continue
                
                # Validate SQL
                if not _is_valid_sql_query(sql_query):
                    continue
                
                try:
                    executed_queries.add(query_key)
                    
                    # Execute the query
                    result = sql_tool.execute_query(sql_query)
                    
                    if result.get("success", False):
                        query_result = result.get("result", [])
                        row_count = result.get("row_count", 0)
                        
                        # Format and return real result
                        real_result = {
                            "type": "sql_execution",
                            "query": sql_query,
                            "success": True,
                            "row_count": row_count,
                            "result": query_result
                        }
                        
                        if query_result:
                            if len(query_result) == 1 and len(query_result[0]) == 1:
                                # Single value result (like COUNT)
                                value = list(query_result[0].values())[0]
                                real_result["value"] = value
                                real_result["message"] = f"**âœ… REAL Result:** {value} (actual count from database)"
                            else:
                                # Multiple rows/columns - format as table
                                formatted_table = _format_query_results_as_table(query_result)
                                real_result["message"] = f"**âœ… REAL Results:** {row_count} rows\n\n{formatted_table}"
                        
                        sql_executed = True
                        return real_result  # Return first successful result
                        
                except Exception as e:
                    logger.error(f"Error executing SQL: {e}")
        
        return None
                        
    except Exception as e:
        logger.warning(f"Error in SQL post-processing: {e}")
        return None
```

**SQL Processing Features**:
- **Multi-pattern Extraction**: Support nhiá»u format SQL code blocks
- **Duplicate Prevention**: TrÃ¡nh execute cÃ¹ng má»™t query nhiá»u láº§n
- **Real Result Detection**: Skip náº¿u Ä‘Ã£ cÃ³ káº¿t quáº£ tháº­t
- **Query Validation**: Validate SQL syntax trÆ°á»›c khi execute
- **Result Formatting**: Format káº¿t quáº£ thÃ nh markdown table Ä‘áº¹p

#### SQL Utility Functions

**`_clean_sql_query()`**:
```python
def _clean_sql_query(sql_query: str):
    """Clean and extract valid SQL query from text"""
    if not sql_query:
        return None
    
    # Remove leading/trailing whitespace
    sql_query = sql_query.strip()
    
    # Remove comments and empty lines
    lines = []
    for line in sql_query.split('\n'):
        line = line.strip()
        if line and not line.startswith('--'):
            lines.append(line)
    
    if not lines:
        return None
    
    # Join lines and clean up
    cleaned = ' '.join(lines)
    cleaned = ' '.join(cleaned.split())  # Remove extra whitespace
    
    # Ensure semicolon at end
    if cleaned and not cleaned.endswith(';'):
        cleaned += ';'
    
    return cleaned
```

**`_is_valid_sql_query()`**:
```python
def _is_valid_sql_query(sql_query: str) -> bool:
    """Validate if the string is a proper SQL query"""
    if not sql_query or len(sql_query) < 8:
        return False
    
    # Check for SQL keywords at start
    sql_keywords = ['SELECT', 'INSERT', 'UPDATE', 'DELETE', 'CREATE', 'DROP', 'ALTER', 'SHOW', 'DESCRIBE', 'EXPLAIN']
    first_word = sql_query.strip().split()[0].upper()
    
    if first_word not in sql_keywords:
        return False
    
    # Skip example or placeholder queries
    forbidden_words = ['example', 'placeholder', 'your_table', 'your_column', 'sample_data']
    if any(word in sql_query.lower() for word in forbidden_words):
        return False
    
    # Skip if it contains explanatory text mixed with SQL
    explanatory_words = ['need to', 'according to', 'explanation', 'this query', 'the result']
    if any(phrase in sql_query.lower() for phrase in explanatory_words):
        return False
    
    return True
```

**`_format_query_results_as_table()`**:
```python
def _format_query_results_as_table(query_results):
    """Format SQL query results as a nice markdown table"""
    if not query_results:
        return "*No data found*"
    
    # Get column names and filter sensitive columns
    first_row = dict(query_results[0])
    columns = list(first_row.keys())
    sensitive_columns = ['password_hash', 'password', 'token', 'secret']
    display_columns = [col for col in columns if not any(sens in col.lower() for sens in sensitive_columns)]
    
    # Limit columns for readability (max 6)
    if len(display_columns) > 6:
        display_columns = display_columns[:5] + ['...more']
    
    # Create markdown table
    table_lines = []
    table_lines.append("| " + " | ".join(display_columns) + " |")
    table_lines.append("| " + " | ".join(["---"] * len(display_columns)) + " |")
    
    # Add rows (limit to 10)
    max_rows = min(10, len(query_results))
    for i in range(max_rows):
        row = dict(query_results[i])
        row_values = []
        
        for col in display_columns:
            if col == '...more':
                row_values.append(f"+{len(columns) - 5} cols")
                continue
                
            value = row.get(col, '')
            
            # Format different data types
            if value is None:
                formatted_value = "*null*"
            elif hasattr(value, 'strftime'):  # datetime objects
                formatted_value = value.strftime("%Y-%m-%d %H:%M")
            elif isinstance(value, str) and len(value) > 30:
                formatted_value = value[:27] + "..."
            elif isinstance(value, bool):
                formatted_value = "âœ“" if value else "âœ—"
            else:
                formatted_value = str(value)
            
            # Escape markdown special characters
            formatted_value = formatted_value.replace("|", "\\|").replace("\n", " ")
            row_values.append(formatted_value)
        
        table_lines.append("| " + " | ".join(row_values) + " |")
    
    # Add summary
    if len(query_results) > max_rows:
        table_lines.append(f"*Showing {max_rows} of {len(query_results)} total rows*")
    
    return "\n".join(table_lines)
```

### 10. **ðŸ” Health Check & Diagnostics**

#### `check_ai_service()` - Service Health Check
```python
async def check_ai_service() -> Dict[str, Any]:
    """Check if LM Studio API is available and return status details"""
    try:
        async with httpx.AsyncClient(timeout=5.0) as client:
            response = await client.get(f"{LM_STUDIO_BASE_URL}/models")
            
            if response.status_code == 200:
                models_data = response.json()
                model_count = len(models_data.get("data", []))
                sample_model = models_data.get("data", [{}])[0].get("id", "unknown") if model_count > 0 else "none"
                
                return {
                    "status": "available",
                    "message": "LM Studio API is available",
                    "base_url": LM_STUDIO_BASE_URL,
                    "model_count": model_count,
                    "sample_model": sample_model
                }
            else:
                return {
                    "status": "error",
                    "message": f"LM Studio API returned status code {response.status_code}",
                    "base_url": LM_STUDIO_BASE_URL
                }
    except Exception as e:
        return {
            "status": "unavailable",
            "message": f"Could not connect to LM Studio API: {str(e)}",
            "base_url": LM_STUDIO_BASE_URL
        }
```

## ðŸš€ Key Features & Innovations

### 1. **Universal Streaming Architecture**
- Táº¥t cáº£ chat functions Ä‘á»u há»— trá»£ streaming
- Direct OpenAI API streaming bypass LangChain limitations
- Real-time token-level response generation
- Minimal memory footprint vá»›i async generators

### 2. **Smart Context Management**
- Auto-detect database-related questions
- Dynamic schema injection vÃ o system prompts
- Conversation history preservation
- Tool context awareness

### 3. **Hybrid Agent/Non-Agent System**
- Seamless switching between agent vÃ  direct LLM calls
- Graceful fallback mechanisms
- Tool integration vá»›i LangChain agents
- Performance optimization cho tá»«ng mode

### 4. **Intelligent SQL Post-Processing**
- Auto-detect vÃ  execute SQL tá»« AI responses
- Multiple SQL format support
- Duplicate prevention system
- Beautiful result formatting

### 5. **Advanced Error Handling**
- Comprehensive retry logic
- Connection failure recovery
- Timeout management
- Graceful degradation

### 6. **Performance Optimizations**
- Singleton pattern cho ChatOpenAI instances
- 5-minute model caching
- Efficient memory management
- Async/await throughout

## ðŸ”§ Usage Examples

### Basic Chat
```python
async for chunk in chat_with_ai(
    message="Hello, how are you?",
    streaming=True
):
    print(chunk, end="")
```

### Agent Chat with Tools
```python
async for chunk in chat_with_ai(
    message="Count how many users are in the database",
    use_agent=True,
    streaming=True,
    tools=sql_tools
):
    print(chunk, end="")
```

### Journal Analysis
```python
result = await analyze_journal_entry(
    entry_title="My Day",
    entry_content="Today was a great day...",
    analysis_type="mood"
)
print(result["answer"])
```

### Writing Improvement
```python
result = await improve_writing(
    text="This is my text that need improvement",
    improvement_type="grammar"
)
print(result["answer"])
```

## ðŸŽ¯ Summary

File `lm_studio.py` lÃ  má»™t AI backend hoÃ n chá»‰nh vá»›i:

- **Streaming-first design** cho real-time responses
- **Hybrid architecture** há»— trá»£ cáº£ agent vÃ  direct LLM calls
- **Smart context management** vá»›i database schema injection
- **Intelligent SQL processing** vá»›i auto-execution
- **Comprehensive error handling** vÃ  retry mechanisms
- **Performance optimizations** vá»›i caching vÃ  singletons
- **Specialized functions** cho journal analysis, writing improvement, etc.

Code Ä‘Æ°á»£c thiáº¿t káº¿ Ä‘á»ƒ scalable, maintainable, vÃ  user-friendly vá»›i comprehensive logging vÃ  error messages.

## ðŸ” THá»°C Táº¾ HIá»†N Táº I VS DOCUMENTATION

### âœ… **ÄÃƒ ÄÃšNG Vá»šI CODE THá»°C Táº¾:**
- File `lm_studio.py` cÃ³ 1040 dÃ²ng code
- Function `chat_with_ai()` lÃ  central orchestrator
- Streaming implementation qua `query_lm_studio_stream()`
- SQL post-processing vá»›i auto-execution
- Agent integration vá»›i LangChain
- All API endpoints trong `/ai` router
- Think/Answer separation
- Database context detection
- Model management vá»›i caching

### âš ï¸ **Cáº¦N Cáº¬P NHáº¬T:**
- Environment variables máº·c Ä‘á»‹nh
- Má»™t sá»‘ parameter names chi tiáº¿t  
- Future features chÆ°a implement
- Performance metrics chÆ°a Ä‘áº§y Ä‘á»§
- Redis caching layer chÆ°a cÃ³
- PGVector integration chÆ°a cÃ³

### ðŸ“‹ **API ENDPOINTS THá»°C Táº¾:**
- `GET /ai/status` - Health check âœ…
- `GET /ai/models` - List models âœ…  
- `POST /ai/chat` - Non-streaming chat âœ…
- `POST /ai/chat-stream` - Streaming chat âœ…
- `POST /ai/analyze-entry` - Journal analysis âœ…
- `POST /ai/improve-writing` - Writing improvement âœ…
- `POST /ai/generate-prompts` - Prompt generation âœ…
- `POST /ai/writing-suggestions` - Writing suggestions âœ…

## ðŸŽ¯ API Endpoints & Integration

### ðŸŒ Complete API Reference (ACTUAL IMPLEMENTATION)

```mermaid
graph TB
    subgraph "AI Service Endpoints - THá»°C Táº¾"
        A["/ai/status"] --> A1["Service Health Check"]
        B["/ai/models"] --> B1["Available Models List"]
        C["/ai/chat"] --> C1["Non-streaming Chat"]
        D["/ai/chat-stream"] --> D1["Streaming Chat vá»›i SSE"]
        E["/ai/analyze-entry"] --> E1["Journal Analysis"]
        F["/ai/improve-writing"] --> F1["Writing Enhancement"]
        G["/ai/generate-prompts"] --> G1["Prompt Generation"]
        H["/ai/writing-suggestions"] --> H1["Writing Suggestions"]
    end
    
    subgraph "Request/Response Flow - THá»°C Táº¾"
        I["Frontend Request"] --> J["FastAPI Router /ai"]
        J --> K["ai.py endpoint handler"]
        K --> L["lm_studio.py functions"]
        L --> M["LM Studio Server :1234"]
        M --> N["AI Model Processing"]
        N --> O["Stream/Response Back"]
    end
    
    I --> A
    I --> B
    I --> C
    I --> D
    I --> E
    I --> F
    I --> G
    I --> H
```

### ðŸ”§ Configuration Management (THá»°C Táº¾)

```mermaid
graph TD
    subgraph "Environment Configuration - HIá»†N Táº I"
        A[".env File"] --> B["LM Studio Settings"]
        A --> C["AI Model Parameters"]
        A --> D["Database Connection"]
        A --> E["Performance Tuning"]
    end
    
    subgraph "LM Studio Settings - THá»°C Táº¾"
        B --> B1["LM_STUDIO_BASE_URL=http://host.docker.internal:1234/v1"]
        B --> B2["LM_STUDIO_API_KEY=not-needed"]
        B --> B3["LM_STUDIO_MODEL=deepseek-r1-distill-qwen-1.5b"]
    end
    
    subgraph "AI Parameters - DEFAULTS"
        C --> C1["DEFAULT_TEMPERATURE=0.7"]
        C --> C2["DEFAULT_MAX_TOKENS=2000"]
        C --> C3["MAX_INFERENCE_TIME=60000ms"]
        C --> C4["DEFAULT_AI_MODEL=qwen/qwen3-1.7b"]
    end
    
    subgraph "Analysis Configuration - THá»°C Táº¾"
        F["ANALYSIS_MAX_TOKENS"] --> F1["general: 800"]
        F --> F2["mood: 800"]
        F --> F3["summary: 600"]
        F --> F4["insights: 700"]
    end
```

## ðŸš€ Performance Optimization & Monitoring

### ðŸ“Š Performance Metrics Dashboard

```mermaid
graph LR
    subgraph "Real-time Metrics"
        A[Token Streaming Rate] --> A1[Tokens/Second]
        B[Inference Time] --> B1[Response Latency]
        C[SQL Execution] --> C1[Query Performance]
        D[Memory Usage] --> D1[Context Management]
    end
    
    subgraph "Health Monitoring"
        E[LM Studio Status] --> E1[Connection Health]
        F[Model Availability] --> F1[Load Status]
        G[Database Connection] --> G1[SQL Tool Status]
    end
    
    subgraph "Error Tracking"
        H[API Errors] --> H1[Error Rates]
        I[Timeout Events] --> I1[Performance Issues]
        J[Retry Statistics] --> J1[Reliability Metrics]
    end
```

### ðŸ”§ Advanced Usage Patterns

#### Batch Processing with AI
```python
async def process_multiple_entries(entries: List[str]):
    """Process multiple journal entries with AI analysis"""
    results = []
    
    for entry in entries:
        try:
            # Analyze each entry
            analysis = await analyze_journal_entry(
                entry_title="Batch Entry",
                entry_content=entry,
                analysis_type="general"
            )
            results.append(analysis)
            
            # Small delay to prevent overload
            await asyncio.sleep(0.1)
            
        except Exception as e:
            logger.error(f"Error processing entry: {e}")
            results.append({"error": str(e)})
    
    return results
```

#### Custom Analysis Pipeline
```python
async def comprehensive_analysis(entry_content: str):
    """Run comprehensive analysis pipeline"""
    analyses = {}
    
    # Run different analysis types in parallel
    tasks = [
        analyze_journal_entry("Entry", entry_content, "general"),
        analyze_journal_entry("Entry", entry_content, "mood"),
        analyze_journal_entry("Entry", entry_content, "summary"),
        analyze_journal_entry("Entry", entry_content, "insights")
    ]
    
    results = await asyncio.gather(*tasks, return_exceptions=True)
    
    analysis_types = ["general", "mood", "summary", "insights"]
    for i, result in enumerate(results):
        if isinstance(result, Exception):
            analyses[analysis_types[i]] = {"error": str(result)}
        else:
            analyses[analysis_types[i]] = result
    
    return analyses
```

## ðŸ›¡ï¸ Security & Best Practices

### ðŸ” Security Implementation

```mermaid
graph TD
    subgraph "Input Validation"
        A[User Input] --> B[Pydantic Validation]
        B --> C[Content Sanitization]
        C --> D[Length Limits]
    end
    
    subgraph "SQL Security"
        E[SQL Queries] --> F[Pattern Validation]
        F --> G[Keyword Filtering]
        G --> H[Injection Prevention]
    end
    
    subgraph "API Security"
        I[Rate Limiting] --> J[Request Throttling]
        K[Authentication] --> L[JWT Validation]
        M[CORS Configuration] --> N[Origin Validation]
    end
    
    subgraph "Data Protection"
        O[Sensitive Data Filtering] --> P[Password Masking]
        Q[Output Sanitization] --> R[Response Cleaning]
    end
```

### ðŸ“ Development Guidelines

#### Error Handling Best Practices
```python
async def robust_ai_call(request_data: dict):
    """Example of robust AI call with comprehensive error handling"""
    try:
        # Validate input
        if not request_data.get("message"):
            raise ValueError("Message is required")
        
        # Process with timeout
        async with asyncio.timeout(60):  # 60 second timeout
            async for chunk in chat_with_ai(
                message=request_data["message"],
                streaming=True
            ):
                yield chunk
                
    except asyncio.TimeoutError:
        yield "Request timed out. Please try a shorter message."
    except ValueError as e:
        yield f"Input validation error: {str(e)}"
    except Exception as e:
        logger.error(f"Unexpected error in AI call: {e}")
        yield f"An unexpected error occurred. Please try again."
```

## ðŸ” Troubleshooting & Diagnostics

### ðŸš¨ Common Issues Resolution

```mermaid
graph TD
    A[Issue Categories] --> B[Connection Issues]
    A --> C[Performance Problems]
    A --> D[AI Response Issues]
    A --> E[SQL Integration Issues]
    
    B --> B1[LM Studio Not Running]
    B --> B2[Wrong Base URL]
    B --> B3[Model Not Loaded]
    
    C --> C1[High Latency]
    C --> C2[Memory Leaks]
    C --> C3[Token Limits Exceeded]
    
    D --> D1[Empty Responses]
    D --> D2[Malformed Output]
    D --> D3[Inconsistent Quality]
    
    E --> E1[Database Connection Failed]
    E --> E2[Schema Loading Issues]
    E --> E3[SQL Execution Errors]
```

### ðŸ”§ Diagnostic Tools

#### Health Check Utility
```python
async def comprehensive_health_check():
    """Comprehensive system health check"""
    health_status = {
        "lm_studio": await check_ai_service(),
        "models": await get_available_models(),
        "database": await check_database_connection(),
        "sql_tools": await test_sql_tools()
    }
    
    # Summarize health
    all_healthy = all(
        status.get("status") == "available" 
        for status in health_status.values() 
        if isinstance(status, dict)
    )
    
    health_status["overall"] = "healthy" if all_healthy else "issues_detected"
    return health_status

async def check_database_connection():
    """Check database connectivity for SQL tools"""
    try:
        database_url = os.getenv("DATABASE_URL")
        if not database_url:
            return {"status": "not_configured", "message": "DATABASE_URL not set"}
        
        from app.ai.sql_tool import PostgreSQLTool
        sql_tool = PostgreSQLTool(database_url)
        schema = sql_tool.get_database_schema()
        
        if schema.get("success"):
            return {"status": "available", "tables": len(schema.get("tables", []))}
        else:
            return {"status": "error", "message": schema.get("error")}
            
    except Exception as e:
        return {"status": "error", "message": str(e)}
```

## ðŸ“š Summary & Future Roadmap

### âœ¨ Current Capabilities

The `lm_studio.py` module provides a comprehensive AI backend with:

- **Universal Streaming Architecture** for real-time responses
- **Intelligent SQL Processing** with auto-execution
- **Multi-modal AI Functions** (chat, analysis, writing improvement)
- **Robust Error Handling** and retry mechanisms
- **Performance Optimization** with caching and connection management
- **Security Features** with input validation and output sanitization

### ðŸš€ Future Enhancements

```mermaid
graph LR
    subgraph "Planned Features"
        A[Vector Database Integration] --> A1[Semantic Search]
        B[Advanced RAG] --> B1[Context Retrieval]
        C[Multi-model Support] --> C1[Model Switching]
        D[Advanced Analytics] --> D1[Usage Metrics]
    end
    
    subgraph "Performance Improvements"
        E[Connection Pooling] --> E1[Better Scalability]
        F[Response Caching] --> F1[Reduced Latency]
        G[Async Optimization] --> G1[Higher Throughput]
    end
    
    subgraph "New AI Capabilities"
        H[Image Analysis] --> H1[Vision Models]
        I[Code Generation] --> I1[Programming Assistant]
        J[Advanced Reasoning] --> J1[Chain of Thought]
    end
```

### ðŸŽ¯ Architecture Benefits

- **Modular Design**: Easy to extend and maintain
- **Performance Focused**: Optimized for real-time applications  
- **Error Resilient**: Comprehensive error handling and recovery
- **User Friendly**: Intuitive APIs with clear error messages
- **Scalable**: Designed to handle multiple concurrent users
- **Secure**: Built-in security features and input validation

The system is designed to be the foundation for advanced AI-powered educational and productivity applications, with a focus on providing excellent user experience through real-time streaming and intelligent automation.
