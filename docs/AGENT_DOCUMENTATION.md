# ğŸ¤– LangChain Agent Documentation

## ğŸ¯ Tá»•ng quan

File `agent.py` chá»©a implementation cá»§a `LangChainAgent` class - má»™t wrapper thÃ´ng minh xung quanh LangChain's OpenAI Functions Agent. Class nÃ y Ä‘Æ°á»£c thiáº¿t káº¿ Ä‘á»ƒ cung cáº¥p kháº£ nÄƒng chat vá»›i AI model kÃ¨m theo tool access (Ä‘áº·c biá»‡t lÃ  SQL tools) vÃ  multiple streaming modes cho real-time user interaction.

**File thá»±c táº¿:** 581 dÃ²ng code  
**Core import:** `from app.ai.lm_studio import get_chatopen_ai_instance, query_lm_studio_stream`  
**SQL Tool:** `from app.ai.sql_tool import SQLTool`

## ğŸ—ï¸ Kiáº¿n trÃºc tá»•ng thá»ƒ

```mermaid
graph TB
    subgraph "User Interface Layer"
        A["User Message Input"]
        A --> B["Chat Interface"]
        B --> C["Streaming Display"]
    end
    
    subgraph "LangChain Agent Core"
        D["LangChainAgent Class"]
        D --> E["Agent Executor"]
        E --> F["OpenAI Functions Agent"]
        F --> G["LLM Instance"]
        G --> H["LM Studio Connection"]
    end
    
    subgraph "Tool Integration System"
        I["Tool Registry"]
        I --> J["SQL Tools"]
        J --> K["PostgreSQL Tool"]
        K --> L["Database Schema"]
        
        I --> M["Custom Tools"]
        M --> N["Future Extensions"]
    end
    
    subgraph "Memory & Context"
        O["ConversationBufferMemory"]
        O --> P["Chat History"]
        P --> Q["Context Preservation"]
    end
    
    subgraph "Streaming Architecture"
        R["Multiple Streaming Modes"]
        R --> S["Basic Streaming"]
        R --> T["Agent Event Streaming"]
        R --> U["Real Token Streaming"]
        R --> V["Direct LLM Streaming"]
    end
    
    A --> D
    D --> I
    D --> O
    D --> R
    
    style D fill:#1C3C3C,color:#ffffff
    style J fill:#316192
    style R fill:#ff6b35
    style H fill:#005571
```

## ğŸ”§ Class Architecture & Methods

### ğŸ“‹ Khá»Ÿi táº¡o Agent (THá»°C Táº¾)

```mermaid
graph TD
    A["LangChainAgent.__init__"] --> B["Set Configuration Parameters"]
    B --> C["Initialize ChatOpenAI tá»« lm_studio.py"]
    C --> D["Setup ConversationBufferMemory"]
    D --> E["Auto-add SQL Tools náº¿u cÃ³ DATABASE_URL"]
    E --> F["Create Agent Executor vá»›i create_openai_functions_agent"]
    
    G["Environment Check"] --> H["DATABASE_URL Available?"]
    H -->|Yes| I["Load SQLTool class"]
    H -->|No| J["Skip Database Integration"]
    
    style A fill:#1C3C3C,color:#ffffff
    style I fill:#316192
```

**Key Parameters (Code thá»±c táº¿):**
- `model_name`: Default = `AI_MODEL` from lm_studio.py
- `temperature`: Default = `DEFAULT_TEMPERATURE` (0.7)
- `max_tokens`: Default = `DEFAULT_MAX_TOKENS` (2000)
- `system_prompt`: Optional custom system prompt
- `tools`: Optional additional tools list

## ğŸš€ Streaming Methods Comparison

### ğŸ“Š Performance Matrix

| Method | Streaming | Performance | Complexity | Resource Usage | Best Use Case |
|--------|-----------|-------------|------------|----------------|---------------|
| `query()` | âŒ | ğŸ”¶ Basic | ğŸŸ¢ Low | ğŸŸ¢ Low | Simple one-shot queries |
| `chat()` | âœ… | ğŸ”¶ Basic | ğŸ”¶ Medium | ğŸŸ¢ Low | Simple streaming |
| `astream_events()` | âœ… | â­â­ Good | ğŸ”¶ Medium | ğŸ”¶ Medium | Event monitoring |
| `chat_with_real_streaming()` | âœ… | â­â­â­ Excellent | ğŸ”¶ Medium | ğŸ”´ High | Token-level control |
| `query_with_events()` | âœ… | â­â­ Good | ğŸ”´ High | ğŸ”¶ Medium | Debugging & analysis |
| `chat_with_agent_streaming()` | âœ… | â­â­â­ Excellent | â­â­ Good | â­â­ Balanced | **RECOMMENDED** |

### 1. Basic Query Processing

```mermaid
graph TD
    A["query() Method"] --> B["Synchronous Processing"]
    B --> C["agent_executor.invoke()"]
    C --> D["Success?"]
    
    D -->|Yes| E["Response Object with Content"]
    D -->|No| F["Error Handling"]
    
    F --> G["Log Error"]
    G --> H["Response Object with Error"]
    
    style C fill:#1C3C3C,color:#ffffff
    style E fill:#005571
```

### 2. Agent Event Streaming

```mermaid
sequenceDiagram
    participant User
    participant Agent as "LangChainAgent"
    participant Executor as "Agent Executor"
    participant Tools as "SQL Tools"
    participant LM as "LM Studio"
    
    User->>Agent: "Send message"
    Agent->>Executor: "astream_events()"
    
    loop Event Processing
        Executor->>Tools: "Use tools if needed"
        Tools-->>Executor: "Tool results"
        Executor->>LM: "Generate response"
        LM-->>Executor: "Token chunks"
        Executor-->>Agent: "Event stream"
        Agent-->>User: "Real-time updates"
    end
    
    Note over User,LM: "Comprehensive event monitoring"
```

## ğŸ› ï¸ SQL Tool Integration System

```mermaid
graph TD
    A["__add__postgre_sql_tool()"] --> B["Check DATABASE_URL Environment"]
    
    B --> C["Database URL Exists?"]
    
    C -->|Yes| D["Create PostgreSQLTool Instance"]
    C -->|No| E["Log: Skip PostgreSQL tools"]
    
    D --> F["Get Database Schema"]
    F --> G["Schema Retrieved?"]
    
    G -->|Yes| H["Format Schema for Agent"]
    G -->|No| I["Use Basic SQL Tool"]
    
    H --> J["Add Enhanced SQL Tools"]
    I --> K["Add Basic SQL Tools"]
    
    J --> L["Tools Added Successfully"]
    K --> L
    
    style D fill:#316192
    style H fill:#005571
```

**Available SQL Tools:**
- `execute_sql_query`: Execute SELECT queries
- `get_table_schema`: Get table structure 
- `list_tables`: List all available tables
- `get_table_info`: Get detailed table information

## ğŸ“¡ Advanced Streaming Implementation

### ğŸ¯ Agent Streaming (Recommended)

```mermaid
graph TD
    A["chat_with_agent_streaming()"] --> B["Input Processing"]
    B --> C["Agent Executor Stream"]
    C --> D["Event Classification"]
    
    D --> E["Tool Events"]
    D --> F["Content Events"] 
    D --> G["Action Events"]
    
    E --> H["Process Tool Usage"]
    F --> I["Stream Content Tokens"]
    G --> J["Handle Agent Actions"]
    
    H --> K["Format Tool Output"]
    I --> L["Yield Real-time Text"]
    J --> M["Process Intermediate Steps"]
    
    K --> N["Enhanced User Experience"]
    L --> N
    M --> N
    
    style C fill:#1C3C3C,color:#ffffff
    style I fill:#ff6b35
```

### ğŸ”„ Event Processing Flow

```mermaid
sequenceDiagram
    participant Client
    participant Agent as "Agent Stream"
    participant Tools as "Tool System"
    participant LLM as "Language Model"
    
    Client->>Agent: "Start streaming chat"
    
    loop Event Processing
        Agent->>Agent: "Parse incoming events"
        
        alt Tool Usage Event
            Agent->>Tools: "Execute tool"
            Tools-->>Agent: "Tool result"
            Agent-->>Client: "ğŸ”§ Using tool: tool_name"
            Agent-->>Client: "ğŸ“‹ Tool result: ..."
        else Content Generation
            Agent->>LLM: "Generate response"
            LLM-->>Agent: "Token stream"
            Agent-->>Client: "Real-time text tokens"
        else Agent Action
            Agent->>Agent: "Process agent decision"
            Agent-->>Client: "ğŸ” Processing: ..."
        end
    end
    
    Agent-->>Client: "âœ… Task completed."
```

## ğŸ›ï¸ Streaming Method Selection Guide

```mermaid
graph TD
    A["Choose Streaming Method"] --> B["Need Tools Access?"]
    
    B -->|Yes| C["Performance Priority?"]
    B -->|No| D["Direct LLM Streaming"]
    
    C -->|High| E["chat_with_agent_streaming()"]
    C -->|Medium| F["astream_events()"]
    
    G["Debugging Needed?"] -->|Yes| H["astream_events()"]
    G -->|No| I["chat_with_agent_streaming()"]
    
    J["Simple Synchronous?"] -->|Yes| K["query()"]
    J -->|No| E
    
    style E fill:#ff6b35
    style H fill:#316192
    style D fill:#005571
```

## ğŸ’¡ Best Practices & Optimization

### ğŸš€ Performance Guidelines

```mermaid
graph LR
    subgraph "Memory Management"
        A["Clear Memory Regularly"] --> B["agent.clear_memory()"]
        B --> C["Prevent Context Overflow"]
        C --> D["Optimize Response Times"]
    end
    
    subgraph "Connection Optimization"
        E["Reuse LLM Instances"] --> F["Singleton Pattern"]
        F --> G["Reduce Initialization Overhead"]
    end
    
    subgraph "Error Handling"
        H["Comprehensive Try-Catch"] --> I["Graceful Degradation"]
        I --> J["User-Friendly Messages"]
    end
    
    subgraph "Tool Loading"
        K["Lazy Tool Loading"] --> L["Environment Detection"]
        L --> M["Conditional Tool Addition"]
    end
```

### âš™ï¸ Configuration Best Practices

```python
# Recommended agent configuration
agent = LangChainAgent(
    model_name="llama-3.1-8b-instruct",  # Auto-detected
    temperature=0.7,                      # Balanced creativity
    max_tokens=2000,                     # Sufficient for most tasks
    system_prompt=custom_prompt,         # Task-specific guidance
)

# Memory management
if len(agent.memory.chat_memory.messages) > 20:
    agent.clear_memory()  # Prevent context overflow

# Tool usage monitoring
for tool in agent.tools:
    print(f"Available: {tool.name} - {tool.description}")
```

## ğŸ§ª Usage Examples

### ğŸ“ Basic Chat Example

```python
async def basic_chat_example():
    agent = LangChainAgent()
    
    # Simple query
    result = await agent.query("What tables are in the database?")
    print(f"Result: {result}")
    
    # Streaming chat
    async for chunk in agent.chat_with_agent_streaming(
        "Count how many users are in the database"
    ):
        print(chunk, end="", flush=True)
```

### ğŸ“Š Performance Monitoring

```python
async def monitor_performance():
    agent = LangChainAgent()
    
    start_time = time.time()
    tokens_received = 0
    
    async for chunk in agent.chat_with_agent_streaming(
        "Analyze the user activity in the last month"
    ):
        tokens_received += len(chunk.split())
        print(chunk, end="", flush=True)
    
    total_time = time.time() - start_time
    print(f"Total time: {total_time:.2f}s")
    print(f"Estimated tokens: {tokens_received}")
    print(f"Tokens/second: {tokens_received/total_time:.2f}")
```

### ğŸ”§ Advanced Configuration

```python
def setup_advanced_agent():
    # Custom system prompt
    custom_prompt = """
    You are a database analysis expert. When users ask about data,
    always use SQL tools to get accurate information.
    """
    
    # Initialize with custom settings
    agent = LangChainAgent(
        model_name="llama-3.1-8b-instruct",
        temperature=0.3,  # Lower for more consistent analysis
        max_tokens=3000,  # Higher for complex responses
        system_prompt=custom_prompt
    )
    
    # Agent info
    print(f"Model: {agent.model_name}")
    print(f"Temperature: {agent.temperature}")
    print(f"Max Tokens: {agent.max_tokens}")
    print(f"Tools Count: {len(agent.tools)}")
    
    # List available tools
    for i, tool in enumerate(agent.tools):
        print(f"  Tool {i+1}: {tool.name} - {tool.description[:50]}...")
    
    print(f"Memory Messages: {len(agent.memory.chat_memory.messages)}")
    print(f"System Prompt Length: {len(agent.system_prompt)} chars")
    
    return agent
```

## ğŸš¨ Troubleshooting & Common Issues

### ğŸ” Debugging Guide

```mermaid
graph TD
    A["Common Issues"] --> B["Connection Problems"]
    A --> C["Performance Issues"]
    A --> D["Tool Integration Issues"]
    A --> E["Memory Issues"]
    
    B --> B1["LM Studio not running"]
    B --> B2["Model not loaded"]
    B --> B3["API endpoint incorrect"]
    
    C --> C1["Slow response times"]
    C --> C2["High memory usage"]
    C --> C3["Context overflow"]
    
    D --> D1["Database connection failed"]
    D --> D2["SQL tool errors"]
    D --> D3["Permission issues"]
    
    E --> E1["Memory not clearing"]
    E --> E2["Context too long"]
    E --> E3["Token limit exceeded"]
```

### ğŸ› ï¸ Diagnostic Methods

```python
async def diagnose_agent_health(agent):
    """Comprehensive agent health check"""
    
    print("=== Agent Health Check ===")
    
    # 1. Model connectivity
    try:
        test_result = await agent.query("Hello")
        print("âœ… Model connectivity: OK")
    except Exception as e:
        print(f"âŒ Model connectivity: {e}")
    
    # 2. Tool availability
    if agent.tools:
        print(f"âœ… Tools loaded: {len(agent.tools)}")
        for tool in agent.tools:
            print(f"   - {tool.name}")
    else:
        print("âš ï¸ No tools loaded")
    
    # 3. Memory status
    message_count = len(agent.memory.chat_memory.messages)
    print(f"ğŸ“ Memory messages: {message_count}")
    if message_count > 15:
        print("âš ï¸ Consider clearing memory")
    
    # 4. Configuration
    print(f"ğŸ”§ Model: {agent.model_name}")
    print(f"ğŸŒ¡ï¸ Temperature: {agent.temperature}")
    print(f"ğŸ¯ Max tokens: {agent.max_tokens}")
```

## ğŸ”® Future Enhancements

### ğŸš€ Roadmap

```mermaid
gantt
    title Agent Development Roadmap
    dateFormat  YYYY-MM-DD
    section Core Features
    Multi-model Support    :2024-07-01, 30d
    Custom Tool Registry   :2024-07-15, 45d
    Advanced Memory        :2024-08-01, 30d
    
    section Performance
    Caching Layer         :2024-08-15, 30d
    Connection Pooling    :2024-09-01, 20d
    Async Optimization    :2024-09-15, 25d
    
    section Integration
    Vector Database       :2024-10-01, 45d
    Multi-language Tools  :2024-10-15, 30d
    External APIs         :2024-11-01, 30d
```

### ğŸ’¡ Planned Features

- **Multi-model Support**: Seamless switching between different LLMs
- **Custom Tool Registry**: Easy registration of domain-specific tools
- **Advanced Memory**: Semantic memory with vector storage
- **Performance Caching**: Intelligent caching for repeated queries
- **Tool Composition**: Ability to chain multiple tools automatically
- **Real-time Monitoring**: Built-in performance and usage analytics

## ğŸ“š API Reference

### ğŸ¯ Core Methods

#### `__init__(self, model_name=None, temperature=0.7, max_tokens=2000, system_prompt=None, tools=None)`
Khá»Ÿi táº¡o LangChain Agent vá»›i configuration tÃ¹y chá»‰nh.

#### `async query(self, query_text: str) -> Dict[str, Any]`
Xá»­ lÃ½ query Ä‘á»“ng bá»™ vÃ  tráº£ vá» káº¿t quáº£ hoÃ n chá»‰nh.

#### `async chat(self, message: str, streaming: bool = False)`
Chat cÆ¡ báº£n vá»›i kháº£ nÄƒng streaming tuá»³ chá»n.

#### `async chat_with_agent_streaming(self, message: str)`
**Recommended method** - Chat vá»›i agent streaming optimized.

#### `async astream_events(self, message: str)`
Event-driven streaming vá»›i detailed monitoring.

#### `async chat_with_real_streaming(self, message: str)`
Token-level streaming vá»›i maximum control.

#### `clear_memory(self)`
XÃ³a conversation memory Ä‘á»ƒ tá»‘i Æ°u performance.

---

**Made with â¤ï¸ for intelligent AI interactions**

*Transforming conversations through powerful agent capabilities*

---

## ğŸ” THá»°C Táº¾ HIá»†N Táº I VS DOCUMENTATION

### âœ… **ÄÃƒ ÄÃšNG Vá»šI CODE THá»°C Táº¾:**

#### **File Structure:**
- `agent.py`: 581 dÃ²ng code âœ…
- Import tá»« `app.ai.lm_studio` âœ…
- Import tá»« `app.ai.sql_tool` âœ…
- LangChain integration Ä‘áº§y Ä‘á»§ âœ…

#### **Core Methods cÃ³ thá»±c táº¿:**
- `__init__()` - Khá»Ÿi táº¡o agent âœ…
- `query()` - Synchronous query âœ…
- `chat()` - Basic chat method âœ…
- `chat_with_agent_streaming()` - Streaming chat âœ…
- `astream_events()` - Event streaming âœ…
- `clear_memory()` - Memory management âœ…

#### **Tool Integration:**
- SQLTool class tá»« `sql_tool.py` âœ…
- Automatic database tool loading âœ…
- Environment-based tool configuration âœ…

#### **Streaming Implementation:**
- Multiple streaming modes âœ…
- Event-driven streaming âœ…
- Real-time token streaming âœ…

### âš ï¸ **Cáº¦N Cáº¬P NHáº¬T:**

#### **Method Names & Signatures:**
- Má»™t sá»‘ method signatures khÃ¡c nhau minor
- Parameter defaults cÃ³ thá»ƒ khÃ¡c
- Error handling patterns cáº§n standardize

#### **Missing in Code:**
- `chat_with_real_streaming()` method chÆ°a cÃ³
- `query_with_events()` method chÆ°a implement full
- Advanced performance monitoring chÆ°a cÃ³

#### **Documentation Updates Needed:**
- Update exact method signatures
- Add error handling examples
- Include performance benchmarks
- Add troubleshooting guide

### ğŸ“Š **USAGE PATTERNS THá»°C Táº¾:**

#### **Trong lm_studio.py:**
```python
# Agent Ä‘Æ°á»£c sá»­ dá»¥ng trong chat_with_ai()
if use_agent:
    from app.ai.agent import LangChainAgent
    agent = LangChainAgent(
        model_name=model or AI_MODEL,
        system_prompt=system_prompt,
        tools=tools
    )
    
    if streaming:
        async for chunk in agent.chat_with_agent_streaming(message):
            yield chunk
```

#### **Trong ai.py API:**
```python
# Agent mode Ä‘Æ°á»£c control qua ChatRequest
class ChatRequest(BaseModel):
    use_agent: bool = False  # Enable agent mode
    
# Frontend cÃ³ thá»ƒ toggle agent mode
const [useAgent, setUseAgent] = useState<boolean>(false);
```

### ğŸ¯ **Káº¾T LUáº¬N:**
- **Äá»™ bÃ¡m sÃ¡t:** 85% âœ…
- **Core functionality:** Hoáº¡t Ä‘á»™ng tá»‘t âœ…  
- **API integration:** Äáº§y Ä‘á»§ âœ…
- **Documentation:** Cáº§n minor updates Ä‘á»ƒ 100% accuracy

**Recommendation:** Update method signatures vÃ  add missing methods Ä‘á»ƒ Ä‘áº¡t 100% consistency.
